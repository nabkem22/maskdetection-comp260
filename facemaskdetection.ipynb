{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image  as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images with facemask labelled 'yes': 690\n",
      "The number of images with facemask labelled 'no': 686\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of images with facemask labelled 'yes':\",len(os.listdir('/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/with_mask')))\n",
    "print(\"The number of images with facemask labelled 'no':\",len(os.listdir('/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/without_mask')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Summary:\n",
      "Number of examples: 1376\n",
      "Percentage of positive examples: 50.145348837209305%, number of pos examples: 690\n",
      "Percentage of negative examples: 49.854651162790695%, number of neg examples: 686\n"
     ]
    }
   ],
   "source": [
    "def data_summary(main_path):\n",
    "    yes_path = main_path + 'with_mask'\n",
    "    no_path = main_path + 'without_mask'\n",
    "\n",
    "    # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n",
    "    m_pos = len(listdir(yes_path))\n",
    "    # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n",
    "    m_neg = len(listdir(no_path))\n",
    "    # number of all examples\n",
    "    m = (m_pos + m_neg)\n",
    "\n",
    "    pos_prec = (m_pos * 100.0) / m\n",
    "    neg_prec = (m_neg * 100.0) / m\n",
    "\n",
    "    print(f\"Number of examples: {m}\")\n",
    "    print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\")\n",
    "    print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\")\n",
    "\n",
    "augmented_data_path = '/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/'\n",
    "print(\"\\nData Summary:\")\n",
    "data_summary(augmented_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    dataset = []\n",
    "\n",
    "    for unitData in os.listdir(SOURCE):\n",
    "        data = SOURCE + unitData\n",
    "        if (os.path.getsize(data) > 0):\n",
    "            dataset.append(unitData)\n",
    "        else:\n",
    "            print('Skipped ' + unitData)\n",
    "            print('Invalid file i.e zero size')\n",
    "\n",
    "    train_set_length = int(len(dataset) * SPLIT_SIZE)\n",
    "    test_set_length = int(len(dataset) - train_set_length)\n",
    "    shuffled_set = random.sample(dataset, len(dataset))\n",
    "    train_set = dataset[0:train_set_length]\n",
    "    test_set = dataset[-test_set_length:]\n",
    "\n",
    "    for unitData in train_set:\n",
    "        temp_train_set = SOURCE + unitData\n",
    "        final_train_set = TRAINING + unitData\n",
    "        copyfile(temp_train_set, final_train_set)\n",
    "\n",
    "    for unitData in test_set:\n",
    "        temp_test_set = SOURCE + unitData\n",
    "        final_test_set = TESTING + unitData\n",
    "        copyfile(temp_test_set, final_test_set)\n",
    "\n",
    "\n",
    "YES_SOURCE_DIR = \"/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/with_mask/\"\n",
    "TRAINING_YES_DIR = \"/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/training/yes1/\"\n",
    "TESTING_YES_DIR = \"/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/testing/yes1/\"\n",
    "NO_SOURCE_DIR = \"/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/without_mask/\"\n",
    "TRAINING_NO_DIR = \"/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/training/no1/\"\n",
    "TESTING_NO_DIR = \"/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/testing/no1/\"\n",
    "split_size = .8\n",
    "split_data(YES_SOURCE_DIR, TRAINING_YES_DIR, TESTING_YES_DIR, split_size)\n",
    "split_data(NO_SOURCE_DIR, TRAINING_NO_DIR, TESTING_NO_DIR, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Splitting Data:\n",
      "The number of images with facemask in the training set labelled 'yes':  552\n",
      "The number of images with facemask in the test set labelled 'yes':  138\n",
      "The number of images without facemask in the training set labelled 'no':  548\n",
      "The number of images without facemask in the test set labelled 'no':  138\n"
     ]
    }
   ],
   "source": [
    "print('\\nAfter Splitting Data:')\n",
    "print(\"The number of images with facemask in the training set labelled 'yes': \", len(os.listdir('/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/training/yes1/')))\n",
    "print(\"The number of images with facemask in the test set labelled 'yes': \", len(os.listdir('/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/testing/yes1/')))\n",
    "print(\"The number of images without facemask in the training set labelled 'no': \", len(os.listdir('/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/training/no1/')))\n",
    "print(\"The number of images without facemask in the test set labelled 'no': \", len(os.listdir('/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/testing/no1/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(100, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(100, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1100 images belonging to 2 classes.\n",
      "Found 276 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/training\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
    "                                                    batch_size=10,\n",
    "                                                    target_size=(150, 150))\n",
    "VALIDATION_DIR = \"/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/dataset/testing\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
    "                                                         batch_size=10,\n",
    "                                                         target_size=(150, 150))\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.4462 - acc: 0.8145WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model-001.model/assets\n",
      "110/110 [==============================] - 65s 592ms/step - loss: 0.4462 - acc: 0.8145 - val_loss: 0.1527 - val_acc: 0.9529\n",
      "Epoch 2/30\n",
      "110/110 [==============================] - 64s 582ms/step - loss: 0.3162 - acc: 0.8709 - val_loss: 0.2070 - val_acc: 0.9420\n",
      "Epoch 3/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.2698 - acc: 0.9000INFO:tensorflow:Assets written to: model-003.model/assets\n",
      "110/110 [==============================] - 65s 592ms/step - loss: 0.2698 - acc: 0.9000 - val_loss: 0.0974 - val_acc: 0.9638\n",
      "Epoch 4/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1966 - acc: 0.9282INFO:tensorflow:Assets written to: model-004.model/assets\n",
      "110/110 [==============================] - 65s 591ms/step - loss: 0.1966 - acc: 0.9282 - val_loss: 0.0574 - val_acc: 0.9783\n",
      "Epoch 5/30\n",
      "110/110 [==============================] - 65s 593ms/step - loss: 0.2214 - acc: 0.9055 - val_loss: 0.0850 - val_acc: 0.9746\n",
      "Epoch 6/30\n",
      "110/110 [==============================] - 64s 584ms/step - loss: 0.1829 - acc: 0.9327 - val_loss: 0.0894 - val_acc: 0.9710\n",
      "Epoch 7/30\n",
      "110/110 [==============================] - 64s 578ms/step - loss: 0.1499 - acc: 0.9336 - val_loss: 0.0739 - val_acc: 0.9819\n",
      "Epoch 8/30\n",
      "110/110 [==============================] - 64s 580ms/step - loss: 0.2133 - acc: 0.9264 - val_loss: 0.1136 - val_acc: 0.9493\n",
      "Epoch 9/30\n",
      "110/110 [==============================] - 64s 583ms/step - loss: 0.1290 - acc: 0.9509 - val_loss: 0.0655 - val_acc: 0.9819\n",
      "Epoch 10/30\n",
      "110/110 [==============================] - 64s 585ms/step - loss: 0.1601 - acc: 0.9455 - val_loss: 0.0582 - val_acc: 0.9674\n",
      "Epoch 11/30\n",
      "110/110 [==============================] - 64s 585ms/step - loss: 0.1324 - acc: 0.9555 - val_loss: 0.0735 - val_acc: 0.9674\n",
      "Epoch 12/30\n",
      "110/110 [==============================] - 62s 565ms/step - loss: 0.1335 - acc: 0.9509 - val_loss: 0.0584 - val_acc: 0.9674\n",
      "Epoch 13/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1082 - acc: 0.9627INFO:tensorflow:Assets written to: model-013.model/assets\n",
      "110/110 [==============================] - 64s 584ms/step - loss: 0.1082 - acc: 0.9627 - val_loss: 0.0408 - val_acc: 0.9783\n",
      "Epoch 14/30\n",
      "110/110 [==============================] - 62s 568ms/step - loss: 0.1208 - acc: 0.9564 - val_loss: 0.0729 - val_acc: 0.9638\n",
      "Epoch 15/30\n",
      "110/110 [==============================] - 62s 566ms/step - loss: 0.0762 - acc: 0.9745 - val_loss: 0.0741 - val_acc: 0.9529\n",
      "Epoch 16/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1055 - acc: 0.9600INFO:tensorflow:Assets written to: model-016.model/assets\n",
      "110/110 [==============================] - 63s 576ms/step - loss: 0.1055 - acc: 0.9600 - val_loss: 0.0178 - val_acc: 1.0000\n",
      "Epoch 17/30\n",
      "110/110 [==============================] - 62s 560ms/step - loss: 0.0819 - acc: 0.9609 - val_loss: 0.0372 - val_acc: 0.9855\n",
      "Epoch 18/30\n",
      "110/110 [==============================] - 62s 561ms/step - loss: 0.1058 - acc: 0.9682 - val_loss: 0.0261 - val_acc: 0.9928\n",
      "Epoch 19/30\n",
      "110/110 [==============================] - 62s 559ms/step - loss: 0.0889 - acc: 0.9682 - val_loss: 0.0896 - val_acc: 0.9601\n",
      "Epoch 20/30\n",
      "110/110 [==============================] - 61s 555ms/step - loss: 0.0822 - acc: 0.9745 - val_loss: 0.1909 - val_acc: 0.9348\n",
      "Epoch 21/30\n",
      "110/110 [==============================] - 61s 553ms/step - loss: 0.1301 - acc: 0.9555 - val_loss: 0.0867 - val_acc: 0.9601\n",
      "Epoch 22/30\n",
      "110/110 [==============================] - 61s 555ms/step - loss: 0.2046 - acc: 0.9355 - val_loss: 0.1610 - val_acc: 0.9457\n",
      "Epoch 23/30\n",
      "110/110 [==============================] - 61s 553ms/step - loss: 0.1448 - acc: 0.9482 - val_loss: 0.1091 - val_acc: 0.9565\n",
      "Epoch 24/30\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1217 - acc: 0.9591INFO:tensorflow:Assets written to: model-024.model/assets\n",
      "110/110 [==============================] - 64s 579ms/step - loss: 0.1217 - acc: 0.9591 - val_loss: 0.0174 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "110/110 [==============================] - 59s 535ms/step - loss: 0.0683 - acc: 0.9773 - val_loss: 0.0188 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "110/110 [==============================] - 60s 543ms/step - loss: 0.0886 - acc: 0.9682 - val_loss: 0.1155 - val_acc: 0.9565\n",
      "Epoch 27/30\n",
      "110/110 [==============================] - 60s 547ms/step - loss: 0.1036 - acc: 0.9636 - val_loss: 0.1170 - val_acc: 0.9674\n",
      "Epoch 28/30\n",
      "110/110 [==============================] - 60s 543ms/step - loss: 0.0807 - acc: 0.9755 - val_loss: 0.1781 - val_acc: 0.9457\n",
      "Epoch 29/30\n",
      "110/110 [==============================] - 59s 537ms/step - loss: 0.1076 - acc: 0.9709 - val_loss: 0.0290 - val_acc: 0.9891\n",
      "Epoch 30/30\n",
      "110/110 [==============================] - 58s 530ms/step - loss: 0.0756 - acc: 0.9755 - val_loss: 0.1247 - val_acc: 0.9638\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                              epochs=40,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr = cv2.CascadeClassifier('/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {0: 'without_mask', 1: 'with_mask'}\n",
    "color_dict = {0: (0, 0, 255), 1: (0, 255, 0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0)  # Use camera 0\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('/Users/nabeelkemal/PycharmProjects/FaceMaskDetection/haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im = cv2.flip(im, 1, 1)  # Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces\n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f]  # Scale the shapesize backup\n",
    "        # Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y + h, x:x + w]\n",
    "        resized = cv2.resize(face_img, (150, 150))\n",
    "        normalized = resized / 255.0\n",
    "        reshaped = np.reshape(normalized, (1, 150, 150, 3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result = model.predict(reshaped)\n",
    "        # print(result)\n",
    "\n",
    "        label = np.argmax(result, axis=1)[0]\n",
    "\n",
    "        cv2.rectangle(im, (x, y), (x + w, y + h), color_dict[label], 2)\n",
    "        cv2.rectangle(im, (x, y - 40), (x + w, y), color_dict[label], -1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE', im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop\n",
    "    if key == 27:  # The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
